{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import logging as log\n",
    "\n",
    "from IPython.display import display\n",
    "\n",
    "import plotly\n",
    "from plotly.offline import init_notebook_mode, iplot\n",
    "plotly.offline.init_notebook_mode() # run at the start of every ipython notebook\n",
    "plotly.plotly.sign_in('spersad', 'MwbO3xbqh2Mv6sfhCma7')\n",
    "import plotly.graph_objs as go\n",
    "\n",
    "log.getLogger().setLevel(log.INFO)\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulate binary vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def simulate_clusters(nclusters, readLength, nreads=10000, probs = None, pi=None, independent = True):\n",
    "    ''' Simulate a mixture of multivariable Bernoulli random variables as binary vectors\n",
    "        @param: nclusters - the number of clusters to generate\n",
    "        @param: readLength - the length of a multivaribable R.V.\n",
    "        @param: nreads - the number of reads to sample\n",
    "        @param: probs - an (optional) list of probability distributions. Must be of length nclusters, and probs[0]\n",
    "                        must have length readLength\n",
    "        @param: pi - the mixing proportions of each cluster. Must be of length nclusters\n",
    "        @param: independent - if this is true, positions are independent of each other. \n",
    "                              Experimentally we find that if there is a mutation at a position, there can be no mutations \n",
    "                              within three bases.\n",
    "        @return: bitvector -  an array of binary vectors\n",
    "    '''\n",
    "    \n",
    "    if probs != None:\n",
    "        if len(pi) != nclusters:\n",
    "            log.error('There is an incorrect number of mixing proportions')\n",
    "        if len(probs) != nclusters:\n",
    "            log.error('There is an incorrect number of probability distributions')\n",
    "            return\n",
    "        for dist in probs:\n",
    "            if len(dist) != readLength:\n",
    "                log.error('The distribution length {0} does not match the read length {1}'.format(len(dist),readLength))\n",
    "                return \n",
    "    else:\n",
    "        # Randomly generate probability distributions \n",
    "        pass \n",
    "    \n",
    "    # Randomly generate binary vectors based on distributions specified\n",
    "\n",
    "    \n",
    "    bitvectors = []\n",
    "    labels = []\n",
    "    for k in range(nclusters):\n",
    "        dist = probs[k]\n",
    "        for j in range(int(nreads*pi[k])):\n",
    "            x = np.array([np.random.binomial(1,p) for p in dist])\n",
    "            if not independent:\n",
    "                for i in range(len(x)-3):\n",
    "                    if x[i]==1:\n",
    "                        x[i+1:i+3]=0 # zero out everyone within a distance of three\n",
    "            if x.sum()>2:\n",
    "                bitvectors.append(x)\n",
    "                labels.append(k)\n",
    "\n",
    "        \n",
    "    # Do some final processing to reshape as needed\n",
    "    bitvectors = np.array(bitvectors)\n",
    "    labels = np.array(labels)\n",
    "    \n",
    "    p = np.random.permutation(len(labels))\n",
    "    \n",
    "    return bitvectors[p], labels[p], probs, pi\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_bitvectors(path, sep='\\t'):\n",
    "    '''\n",
    "    Given a pandas dataframe with headings Read_name Binary_vector N_mutations Reference_name Start_position\n",
    "    load only the bitvectors as a numpy array of integers\n",
    "    @param: path - path to bitvector file\n",
    "    @return: bitreads - an array of bitvectors\n",
    "    '''\n",
    "    bitvectors=pd.read_csv(path,sep=sep)\n",
    "    #Grab the bit vectors only \n",
    "    bitreads = bitvectors.as_matrix(columns=['Binary_vector'])\n",
    "    log.debug(\"bitreads[0][0]=%s\" % ( bitreads[0][0]))\n",
    "    len_bits = str(len(list(bitreads[0][0])))\n",
    "    log.debug(\"Len bits is %s\" % len_bits)\n",
    "    size = bitreads.size\n",
    "    bitreads = np.array(bitreads,dtype='|S'+len_bits)\n",
    "    bitreads = bitreads.view('S1')\n",
    "    bitreads = bitreads.reshape((size, -1))\n",
    "    bitreads = bitreads.astype('|S4')\n",
    "    bitreads[bitreads == '?'] = 0    \n",
    "    bitreads = bitreads.astype(np.float)\n",
    "\n",
    "    \n",
    "    return bitreads\n",
    "\n",
    "\n",
    "def denoise_bitvectors(bitvectors, threshold=0.001):\n",
    "    '''\n",
    "    Given an array of bitvectors, zero out all columns where the population average is below the given threshold.\n",
    "    @param: bitvectors - numpy array of bitvectors\n",
    "    @param: decimal threshold below which entries are considered noise. Default to 0.001=1%\n",
    "    \n",
    "    @return: denoised_bitvectors - numpy array of bitvectors where low signal positions are zero-ed out.'''\n",
    "    population_average = bitvectors.sum(axis=0)/bitvectors.shape[1]\n",
    "    denoise = np.where(population_average<threshold)\n",
    "    bitvectors[:,denoise] = 0\n",
    "    \n",
    "    return bitvectors\n",
    "    \n",
    "    \n",
    "def load_oligo(path):\n",
    "    '''\n",
    "    Given a pandas dataframe with headings Read_name Binary_vector N_mutations Reference_name Start_position\n",
    "    load only the bitvectors as a numpy array of integers\n",
    "    @param: path - path to bitvector file\n",
    "    @return: bitreads - an array of bitvectors\n",
    "    '''\n",
    "    bitvectors=pd.read_csv(path,sep='\\t')\n",
    "    print(bitvectors.head())\n",
    "#     bitvectors = bitvectors[bitvectors['N_mutations'] > 1]\n",
    "    #Grab the bit vectors only \n",
    "    bitreads = bitvectors.as_matrix(columns=['Binary_vector'])\n",
    "    text = bitreads\n",
    "    labels = bitvectors['Reference_name']\n",
    "    labels[labels=='oligo'] = -1\n",
    "    labels[labels=='no_oligo'] = 1\n",
    "    log.debug(\"bitreads[0][0]=%s\" % ( bitreads[0][0]))\n",
    "    len_bits = str(len(list(bitreads[0][0])))\n",
    "    log.debug(\"Len bits is %s\" % len_bits)\n",
    "    size = bitreads.size\n",
    "    bitreads = np.array(bitreads,dtype='|S'+len_bits)\n",
    "    bitreads = bitreads.view('S1')\n",
    "    bitreads = bitreads.reshape((size, -1))\n",
    "    bitreads = bitreads.astype('|S4')\n",
    "    bitreads[bitreads == '?'] = 0    \n",
    "    bitreads = bitreads.astype(np.float)\n",
    "    \n",
    "    idx = np.sum(bitreads, axis=1) > 5 # only allow sufficient mutations\n",
    "    bitreads = bitreads[idx] \n",
    "    labels = labels[idx]\n",
    "    text = text[idx]\n",
    "    p = np.random.permutation(len(labels))\n",
    "    \n",
    "    return bitreads[p], labels[p], text[p]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "l_12=np.array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.7, 0, 0, 0, 0, 0.3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.15, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.4, 0, 0.2, 0, 0, 0.5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.3, 0, 0, 0, 0, 0, 0, 0.005, 0, 0.3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.3, 0, 0, 0.15, 0, 0, 0, 0, 0.0, 0, 0, 0, 0, 0.00, 0, 0, 0, 0, 0.0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.0000, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.3, 0, 0, 0, 0, 0.6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.2, 0, 0, 0, 0, 0, 0, 0, 0, 0.3, 0, 0, 0.2, 0, 0, 0, 0, 0, 0, 0, 0.15, 0, 0, 0, 0, 0, 0, 0, 0, 0.2, 0, 0, 0, 0, 0, 0.15, 0, 0.5, 0.3, 0, 0, 0, 0, 0, 0.5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.2, 0, 0, 0, 0, 0, 0, 0.15, 0, 0, 0.6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.2, 0, 0.5, 0, 0, 0, 0, 0.2, 0, 0, 0, 0, 0, 0, 0, 0.3, 0, 0.3, 0, 0, 0, 0.7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.2, 0, 0, 0, 0.7, 0, 0, 0, 0, 0, 0, 0, 0, 0.3, 0, 0, 0, 0, 0.6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.2, 0])\n",
    "l_11=np.array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.2, 0, 0, 0, 0, 0, 0, 0, 0.15, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.6, 0, 0, 0, 0, 0, 0.2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.0, 0, 0, 0, 0, 0, 0, 0.15, 0, 0, 0, 0, 0.4, 0, 0, 0, 0, 0.2, 0, 0, 0, 0, 0, 0, 0.0, 0, 0, 0.00, 0.45, 0, 0, 0, 0.5, 0, 0, 0, 0, 0.15, 0, 0, 0, 0, 0.3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.2, 0, 0, 0, 0, 0, 0, 0, 0, 0.3, 0, 0, 0.2, 0, 0, 0, 0, 0, 0, 0, 0.15, 0, 0, 0, 0, 0, 0, 0, 0, 0.2, 0, 0, 0, 0.2, 0, 0, 0, 0, 0, 0, 0, 0, 0.2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.6, 0, 0, 0, 0, 0.4, 0, 0, 0.2, 0, 0, 0, 0.2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.7, 0, 0, 0, 0, 0, 0, 0, 0.3, 0, 0.4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.1, 0, 0, 0.3, 0, 0, 0, 0, 0.5, 0, 0, 0, 0, 0.2, 0, 0, 0, 0, 0, 0, 0, 0.3, 0, 0.3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.2, 0, 0, 0, 0.7, 0, 0, 0, 0, 0, 0, 0, 0, 0.3, 0, 0, 0, 0, 0.6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.2, 0])\n",
    "\n",
    "l_22=np.array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.7, 0, 0, 0, 0, 0.3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.15, 0, 0, 0, 0, 0.2, 0, 0, 0, 0, 0, 0, 0, 0.15, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.4, 0, 0.2, 0, 0, 0.5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.3, 0, 0, 0, 0, 0, 0, 0.15, 0, 0.3, 0, 0, 0.4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.3, 0, 0, 0.15, 0, 0, 0, 0, 0.5, 0, 0, 0, 0, 0.15, 0, 0, 0, 0, 0.3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.3, 0, 0, 0, 0, 0.6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.2, 0, 0, 0, 0, 0, 0, 0, 0, 0.3, 0, 0, 0.2, 0, 0, 0, 0, 0, 0, 0, 0.15, 0, 0, 0, 0, 0, 0, 0, 0, 0.2, 0, 0, 0, 0, 0, 0.15, 0, 0.5, 0.3, 0, 0, 0, 0, 0, 0.5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.2, 0, 0, 0, 0, 0, 0, 0.15, 0, 0, 0.6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.2, 0, 0.5, 0, 0, 0, 0, 0.2, 0, 0, 0, 0, 0, 0, 0, 0.3, 0, 0.3, 0, 0, 0, 0.7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.2, 0, 0, 0, 0.7, 0, 0, 0, 0, 0, 0, 0, 0, 0.3, 0, 0, 0, 0, 0.6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.2, 0])\n",
    "l_21=np.array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.7, 0, 0, 0, 0, 0.3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.15, 0, 0, 0, 0, 0.2, 0, 0, 0, 0, 0, 0, 0, 0.15, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.6, 0, 0, 0, 0, 0, 0.2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.3, 0, 0, 0, 0, 0, 0, 0.15, 0, 0, 0, 0, 0.4, 0, 0, 0, 0, 0.2, 0, 0, 0, 0, 0, 0, 0.3, 0, 0, 0.15, 0, 0, 0, 0, 0.5, 0, 0, 0, 0, 0.15, 0, 0, 0, 0, 0.3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.3, 0, 0, 0, 0, 0.6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.2, 0, 0, 0, 0, 0, 0, 0, 0, 0.3, 0, 0, 0.2, 0, 0, 0, 0, 0, 0, 0, 0.15, 0, 0, 0, 0, 0, 0, 0, 0, 0.2, 0, 0, 0, 0.2, 0, 0, 0, 0, 0, 0, 0, 0, 0.2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.6, 0, 0, 0, 0, 0.4, 0, 0, 0.2, 0, 0, 0, 0.2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.7, 0, 0, 0, 0, 0, 0, 0, 0.3, 0, 0.4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.1, 0, 0, 0.3, 0, 0, 0, 0, 0.5, 0, 0, 0, 0, 0.2, 0, 0, 0, 0, 0, 0, 0, 0.3, 0, 0.3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.2, 0, 0, 0, 0.7, 0, 0, 0, 0, 0, 0, 0, 0, 0.3, 0, 0, 0, 0, 0.6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.2, 0])\n",
    "\n",
    "probs_1=[l_11,l_12]\n",
    "pi_1 = [0.3,0.7]\n",
    "\n",
    "probs_2 = [l_21,l_22]\n",
    "pi_2 = [0.3,0.7]\n",
    "\n",
    "probs_3 = [l_11/3, l_12/4]\n",
    "pi_3 = pi_1\n",
    "\n",
    "probs_4 = [l_21/3, l_22/4]\n",
    "pi_4=pi_2\n",
    "\n",
    "simX1, simY1, _,_, = simulate_clusters(nclusters=2, readLength=400, nreads=50000, probs=probs_1, pi=pi_1, independent=False)\n",
    "simX1 = denoise_bitvectors(simX1)\n",
    "log.info('Generated clusters on sample 1.')\n",
    "\n",
    "simX2, simY2, _,_, = simulate_clusters(nclusters=2, readLength=400, nreads=50000, probs=probs_2, pi=pi_2, independent=False)\n",
    "simX2 = denoise_bitvectors(simX2)\n",
    "log.info('Generated clusters on sample 2.')\n",
    "\n",
    "simX3, simY3, _,_, = simulate_clusters(nclusters=2, readLength=400, nreads=50000, probs=probs_3, pi=pi_3, independent=False)\n",
    "simX3 = denoise_bitvectors(simX3)\n",
    "log.info('Generated clusters on sample 3.')\n",
    "\n",
    "simX4, simY4, _,_, = simulate_clusters(nclusters=2, readLength=400, nreads=50000, probs=probs_4, pi=pi_4, independent=False)\n",
    "simX4 = denoise_bitvectors(simX4)\n",
    "log.info('Generated clusters on sample 4.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_clusters(X, Y, sample='1'):\n",
    "    ''' Plot the population average as well as individual clusters.'''\n",
    "    x=[i for i in range(X.shape[1])]\n",
    "    y=100*X.sum(axis=0)/X.shape[0]\n",
    "\n",
    "    trace = go.Bar(\n",
    "        x=x,\n",
    "        y=y,\n",
    "        name='Population Average',\n",
    "        marker = dict(\n",
    "        color='rgb(255, 201, 43)'),\n",
    "    ) \n",
    "    \n",
    "    ## Now plot clusters separately\n",
    "    from plotly import tools\n",
    "    clusters = (np.unique(Y))\n",
    "    \n",
    "    fig2 = tools.make_subplots(rows=len(clusters)+1, cols=1)  \n",
    "    \n",
    "    for cluster in clusters:\n",
    "        cluster_X = X[np.where(Y == cluster)]\n",
    "        \n",
    "        trace1 = go.Bar(\n",
    "        x=x,\n",
    "        y=100*cluster_X.sum(axis=0)/cluster_X.shape[0],\n",
    "        name='Cluster '+str(cluster+1),\n",
    "        marker = dict(\n",
    "        color='rgb(216, 67, 77)'),\n",
    "        )\n",
    "\n",
    "        fig2.append_trace(trace1, cluster+1, 1)\n",
    "        \n",
    "    fig2.append_trace(trace, cluster+2, 1)\n",
    "    fig2['layout'].update(title='Clusters within Sample '+str(sample))\n",
    "    iplot(fig2, filename=\"clusters\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_clusters(simX1, simY1, sample=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_clusters(simX2, simY2, sample=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_clusters(simX3, simY3, sample=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_clusters(simX4, simY4, sample=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "#### t-SNE Visualization\n",
    "\n",
    "Now, we wish to visualize our cluster data when embedded in a lower dimensional space.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def performTSNE(X, Y, num_examples=2000, perp=30):\n",
    "    from sklearn.manifold import TSNE\n",
    "\n",
    "    model = TSNE(n_components=2, perplexity = perp, random_state=0) # fit into 2D space\n",
    "    log.info('Defined model')\n",
    "    embeddedX = model.fit_transform(X[:num_examples])\n",
    "    log.info('Created embedding')\n",
    "\n",
    "    # Scatter plot to visualize embedded data\n",
    "    # Create a trace\n",
    "    trace = go.Scatter(\n",
    "        x = embeddedX[:,0],\n",
    "        y = embeddedX[:,1],\n",
    "        mode = 'markers',\n",
    "        marker=dict(\n",
    "            size='2',\n",
    "            color = Y[:num_examples], # color points by label they belong to\n",
    "            colorscale= [[0, '#dd2c4f'], [1, '#3d6fcc']],\n",
    "        )\n",
    "    )\n",
    "\n",
    "    data = [trace]\n",
    "    \n",
    "    layout = go.Layout(\n",
    "        title='Embedding of Clusters in 2D Space',\n",
    "    )\n",
    "\n",
    "    fig = go.Figure(data=data, layout=layout)\n",
    "    iplot(fig, filename='t-SNE-embedding')\n",
    "    \n",
    "    log.info('Plotted data')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "performTSNE(simX1, simY1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "performTSNE(simX2, simY2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "performTSNE(simX3, simY3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "performTSNE(simX4, simY4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "#### PCA\n",
    "\n",
    "Now we wish to see whether PCA can recover the correct number of clusters in our data, as well as whether the dimensionality reduction yields cluster separation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def performPCA(X, Y, num_examples=2000):\n",
    "    from sklearn.decomposition import PCA\n",
    "\n",
    "    pca = PCA()\n",
    "    log.info('Defined PCA Model')\n",
    "    reducedX = pca.fit_transform(X[:num_examples])\n",
    "    log.info('Fit X to Model')\n",
    "\n",
    "    # Create a trace\n",
    "    trace = go.Scatter(\n",
    "        x = reducedX[:,0],\n",
    "        y = reducedX[:,1],\n",
    "        mode = 'markers',\n",
    "        marker=dict(\n",
    "            size='2',\n",
    "            color = Y[:num_examples], # color points by label they belong to\n",
    "            colorscale= [[0, '#dd2c4f'], [1, '#3d6fcc']],\n",
    "        )\n",
    "    )\n",
    "\n",
    "    data = [trace]\n",
    "\n",
    "    # Plot and embed in ipython notebook!\n",
    "    iplot(data, filename='basic-scatter')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "performPCA(simX1, simY1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "performPCA(simX2, simY2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "performPCA(simX3, simY3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "performPCA(simX4, simY4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "### Stacked Autoencoder\n",
    "\n",
    "Now we wish to see how well an autoencoder can recover the original data from a lower dimensional embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_autoencoder(simX, simY, n_hidden_1=256, n_hidden_2=128, learning_rate=0.01, training_epochs=200, BATCH_SIZE=200):\n",
    "    import matplotlib.pyplot as plt\n",
    "    %matplotlib inline\n",
    "\n",
    "    # Parameters\n",
    "    display_step = 100\n",
    "    n_input = simX.shape[1]   # length of input read\n",
    "    \n",
    "    X = tf.placeholder(\"float64\", [None, n_input])\n",
    "\n",
    "    encoder_W1 = tf.Variable(tf.random_normal([n_input, n_hidden_1], dtype=\"float64\"), name=\"encoder_W1\")\n",
    "    encoder_W2 =  tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2], dtype=\"float64\"), name=\"encoder_W2\")\n",
    "    weights = {\n",
    "        'decoder_h1': tf.Variable(tf.random_normal([n_hidden_2, n_hidden_1], dtype=\"float64\")),\n",
    "        'decoder_h2': tf.Variable(tf.random_normal([n_hidden_1, n_input], dtype=\"float64\")),\n",
    "    }\n",
    "\n",
    "    encoder_b1 = tf.Variable(tf.random_normal([n_hidden_1], dtype=\"float64\"), name=\"encoder_b1\"),\n",
    "    encoder_b2 = tf.Variable(tf.random_normal([n_hidden_2], dtype=\"float64\"), name=\"encoder_b2\"),\n",
    "    biases = {\n",
    "        'decoder_b1': tf.Variable(tf.random_normal([n_hidden_1], dtype=\"float64\")),\n",
    "        'decoder_b2': tf.Variable(tf.random_normal([n_input], dtype=\"float64\")),\n",
    "    }\n",
    "\n",
    "\n",
    "    # Building the encoder\n",
    "    def encoder(x):\n",
    "        # Encoder Hidden layer with sigmoid activation #1\n",
    "        layer_1 = tf.nn.sigmoid(tf.add(tf.matmul(x, encoder_W1),\n",
    "                                       encoder_b1))\n",
    "        # Decoder Hidden layer with sigmoid activation #2\n",
    "        layer_2 = tf.nn.sigmoid(tf.add(tf.matmul(layer_1, encoder_W2),\n",
    "                                       encoder_b2))\n",
    "        return layer_2\n",
    "\n",
    "\n",
    "    # Building the decoder\n",
    "    def decoder(x):\n",
    "        # Encoder Hidden layer with ReLU activation #1\n",
    "        layer_1 = tf.nn.sigmoid(tf.add(tf.matmul(x, weights['decoder_h1']),\n",
    "                                       biases['decoder_b1']))\n",
    "        # Decoder Hidden layer with ReLU activation #2\n",
    "        layer_2 = tf.nn.sigmoid(tf.add(tf.matmul(layer_1, weights['decoder_h2']),\n",
    "                                       biases['decoder_b2']))\n",
    "        return layer_2\n",
    "\n",
    "    # Construct model\n",
    "    encoder_op = encoder(X)\n",
    "    decoder_op = decoder(encoder_op)\n",
    "\n",
    "    # Prediction\n",
    "    y_pred = decoder_op\n",
    "    # Targets (Labels) are the input data.\n",
    "    y_true = X\n",
    "\n",
    "    # Define loss and optimizer, minimize the squared error\n",
    "    cost = tf.reduce_mean(tf.pow(y_true - y_pred, 2))\n",
    "    optimizer = tf.train.RMSPropOptimizer(learning_rate).minimize(cost)\n",
    "\n",
    "    # Initializing the variables\n",
    "    init = tf.global_variables_initializer()\n",
    "    \n",
    "    NUM_BATCHES = simX.shape[0]//BATCH_SIZE\n",
    "\n",
    "    costs = []\n",
    "    # Launch the graph\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init)\n",
    "        # Training cycle\n",
    "        for epoch in range(training_epochs):\n",
    "            # Loop over all batches\n",
    "            for i in range(NUM_BATCHES):\n",
    "                batch_xs= simX[BATCH_SIZE*i : BATCH_SIZE*(i+1)]\n",
    "                # Run optimization op (backprop) and cost op (to get loss value)\n",
    "                _, c = sess.run([optimizer, cost], feed_dict={X: batch_xs})\n",
    "                costs.append(c)\n",
    "            # Display logs per epoch step\n",
    "            if epoch % display_step == 0:\n",
    "                print(\"Epoch:\", '%04d' % (epoch),\n",
    "                      \"cost=\", \"{:.9f}\".format(c))\n",
    "\n",
    "        print(\"Optimization Finished!\")\n",
    "         # Applying encode and decode over test set\n",
    "        encoded, encode_decode = sess.run(\n",
    "            [encoder_op, y_pred], feed_dict={X: simX})\n",
    "        \n",
    "        examples_to_show = 10\n",
    "        # Compare original images with their reconstructions\n",
    "        f, a = plt.subplots(2, examples_to_show, figsize=(examples_to_show, 2))\n",
    "        for i in range(examples_to_show):\n",
    "            a[0][i].imshow(np.reshape(simX[i], (20,20)))\n",
    "            a[1][i].imshow(np.reshape(encode_decode[i], (20,20)))\n",
    "        plt.draw()\n",
    "\n",
    "        encode_W1_vals = sess.run([v for v in tf.global_variables() if v.name == \"encoder_W1:0\"][0])\n",
    "        encode_W2_vals = sess.run([v for v in tf.global_variables() if v.name == \"encoder_W2:0\"][0])\n",
    "        \n",
    "        encode_b1_vals = sess.run([v for v in tf.global_variables() if v.name == \"encoder_b1:0\"][0])\n",
    "        encode_b2_vals = sess.run([v for v in tf.global_variables() if v.name == \"encoder_b2:0\"][0])\n",
    "     \n",
    "    print('Encoded data has dimension {0} and {1} samples.'.format(encoded.shape[1],encoded.shape[0]))\n",
    "    ## Perform t-SNE to determine whether the embedded points are still close in the new low dimensional space\n",
    "    performTSNE(encoded, simY)\n",
    "    return costs, encoded, n_hidden_2, encode_W1_vals, encode_W2_vals, encode_b1_vals, encode_b2_vals\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Embedded Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We discard the decoding function from our autoencoder, and use the learned encoding weights to map our data from the higher dimensional space into our feature space\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_DEC(simX, simY, encoding=True, learning_rate = 1e-3, N_CLUSTERS=2, ALPHA=1, DEC_training_steps=2000):\n",
    "\n",
    "    tf.reset_default_graph()\n",
    "  \n",
    "    n_samples = simX.shape[0]\n",
    "\n",
    "    # Create a placeholder for inputting training X data \n",
    "    input_X = tf.placeholder(\"float64\", [None, simX.shape[1]], name=\"Input_X\")\n",
    "\n",
    "    if encoding=='ae':\n",
    "        # Run auto-encoder to generate embedded data\n",
    "        costs, encoded, DIM_Z, encode_W1_vals, encode_W2_vals, encode_b1_vals, encode_b2_vals = run_autoencoder(simX,simY)\n",
    "\n",
    "        # Building the encoder\n",
    "        encoder_W1 = tf.Variable(encode_W1_vals, name=\"encoder_W1\")\n",
    "        encoder_W2 =  tf.Variable(encode_W2_vals, name=\"encoder_W2\")\n",
    "        encoder_b1 = tf.Variable(encode_b1_vals, name=\"encoder_b1\"),\n",
    "        encoder_b2 = tf.Variable(encode_b2_vals, name=\"encoder_b2\"),   \n",
    "        def encoder(x):\n",
    "            # Encoder Hidden layer with sigmoid activation #1\n",
    "            layer_1 = tf.nn.sigmoid(tf.add(tf.matmul(x, encoder_W1),\n",
    "                                        encoder_b1))\n",
    "            # Decoder Hidden layer with sigmoid activation #2\n",
    "            layer_2 = tf.nn.sigmoid(tf.add(tf.matmul(layer_1, encoder_W2),\n",
    "                                           encoder_b2))\n",
    "            return layer_2\n",
    "        X = encoder(input_X)\n",
    "\n",
    "    elif encoding=='tsne':\n",
    "        print('Performing TSNE')\n",
    "        DIM_Z = 2\n",
    "        from sklearn.manifold import TSNE\n",
    "        model = TSNE(n_components=N_CLUSTERS, perplexity = 30, random_state=0) # fit into 2D space\n",
    "        encoded = model.fit_transform(simX)\n",
    "        X = encoded\n",
    "    else:\n",
    "        X = input_X\n",
    "        encoded = simX\n",
    "        DIM_Z = simX.shape[1]\n",
    "        \n",
    "    # Initialize mu by k means, return a numpy array of shape (N_CLUSTERS, DIM_Z)\n",
    "    from sklearn.cluster import KMeans\n",
    "    kmeans = KMeans(n_clusters=N_CLUSTERS, random_state=0).fit(encoded)\n",
    "    mu_init = kmeans.cluster_centers_\n",
    "    \n",
    "    print('Initialize {0} clusters with dimensions {1}'.format(N_CLUSTERS, encoded.shape[1]))\n",
    "    # Create another Variable initialized with the original cluster centers.\n",
    "    Mu = tf.Variable(mu_init, name=\"Input_Mu\", dtype=\"float64\")\n",
    "    \n",
    "    \n",
    "    X_1 = tf.reshape(X, [n_samples, DIM_Z]) \n",
    "    c = tf.tile(X_1, tf.constant([1, N_CLUSTERS]))\n",
    "    X_reshape = tf.reshape(c,[n_samples,N_CLUSTERS,DIM_Z], name=\"X_reshape\")\n",
    "\n",
    "    #Reshape Mu into a dimension befitting the number of data points\n",
    "    Mu_tile = tf.tile(Mu, tf.constant([n_samples, 1]))\n",
    "    Mu_reshape = tf.reshape(Mu_tile, [n_samples,N_CLUSTERS,DIM_Z], name=\"Mu_reshape\")\n",
    "    Q = tf.pow((tf.squared_difference(Mu_reshape, X_reshape) + 1)/ALPHA, -0.5*(ALPHA+1))\n",
    "\n",
    "    Q_sum = tf.reduce_sum(Q, 2)\n",
    "    Q_norm = Q_sum/tf.reduce_sum(Q_sum, 1, keep_dims=True) ## FINAL Q_ij matrix, where position (i,j) is q_ij\n",
    "\n",
    "    # Compute auxiliary probability distribution P\n",
    "\n",
    "    # p_ij = q_ij^2/f_j , where f_j is the sum of q_ij over i (column-sum)\n",
    "    P = tf.pow(Q_norm,2)/ tf.reduce_sum(Q_norm, 0)\n",
    "    # Now normalize by cluster (row)\n",
    "    P_norm = P/tf.reduce_sum(P, 1, keep_dims=True) ## This gives the responsibilities of each cluster\n",
    "\n",
    "    KL_matrix = tf.multiply(P_norm, tf.log(tf.div(P_norm, Q_norm)))\n",
    "    cost = tf.reduce_sum(KL_matrix)\n",
    "\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)\n",
    "\n",
    "    # Initializing the variables\n",
    "    init = tf.global_variables_initializer()\n",
    "\n",
    "    costs = []\n",
    "    similarities = []\n",
    "    probabilities = []\n",
    "    clusters = []\n",
    "    # Launch the graph\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init)\n",
    "        for i in range(DEC_training_steps):\n",
    "            _, c, q, p, m = sess.run([optimizer, cost, Q_norm, P_norm, Mu], feed_dict={input_X:simX})\n",
    "            if (i%100==1):\n",
    "                print('Cost at epoch {0} : {1}'.format(i, c))\n",
    "                costs.append(c)\n",
    "                similarities.append(q)\n",
    "                probabilities.append(p)\n",
    "                clusters.append(m)\n",
    "    preds = np.argmax(probabilities[-1], axis=1)\n",
    "    acc = 100*sum(simY==preds)/len(preds)\n",
    "    print('Final accuracy: {0}'.format(acc))\n",
    "    return probabilities, clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialize 2 clusters with dimensions 400\n",
      "Cost at epoch 1 : 0.006858604132841612\n",
      "Cost at epoch 101 : 0.002491143873408541\n",
      "Cost at epoch 201 : 0.0006248783112953651\n",
      "Cost at epoch 301 : 0.00014176738793498134\n",
      "Cost at epoch 401 : 6.328116813108397e-05\n",
      "Cost at epoch 501 : 4.489478706828787e-05\n",
      "Cost at epoch 601 : 3.370880236625576e-05\n",
      "Cost at epoch 701 : 2.5206366134369868e-05\n",
      "Cost at epoch 801 : 1.8744713824328486e-05\n",
      "Cost at epoch 901 : 1.3894494152214346e-05\n",
      "Cost at epoch 1001 : 1.0286975873026244e-05\n",
      "Cost at epoch 1101 : 7.6175374445134724e-06\n",
      "Cost at epoch 1201 : 5.645119615865977e-06\n",
      "Cost at epoch 1301 : 4.185743924666404e-06\n",
      "Cost at epoch 1401 : 3.1028334017987695e-06\n",
      "Cost at epoch 1501 : 2.2967816662316533e-06\n",
      "Cost at epoch 1601 : 1.6954701423603892e-06\n",
      "Cost at epoch 1701 : 1.246572330445665e-06\n",
      "Cost at epoch 1801 : 9.117912683694762e-07\n",
      "Cost at epoch 1901 : 6.627741430858728e-07\n"
     ]
    }
   ],
   "source": [
    "p,c = run_DEC(simX1[:1000], simY1[:1000], encoding='none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.50005576,  0.49994424],\n",
       "       [ 0.50003486,  0.49996514],\n",
       "       [ 0.50006288,  0.49993712],\n",
       "       ..., \n",
       "       [ 0.50006441,  0.49993559],\n",
       "       [ 0.50003927,  0.49996073],\n",
       "       [ 0.5001098 ,  0.4998902 ]])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = np.argmax(p[-1], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18.800000000000001"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "100*sum(simY1[:1000]==preds)/len(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
